{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from splitcross import SplitCrossEntropyLoss\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "\n",
    "import data\n",
    "import os\n",
    "from utils import batchify\n",
    "from argparse import Namespace\n",
    "from model import AWDRNNModel\n",
    "from train import train, evaluate\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = []\n",
    "for fn in os.listdir('train_logs_single_run'):\n",
    "    if fn.endswith('.json'):\n",
    "        all_stats.append(json.load(open(os.path.join('train_logs_single_run', fn), 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_ppl = np.array([x['test_losses'][-1] if len(x['test_losses']) > 0 and not np.isnan(x['test_losses'][-1]) else np.inf for x in all_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i = np.argmin(final_test_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.89023051795395"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(all_stats[best_i]['test_losses'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 'data/ptb',\n",
       " 'recepie_id': 1000001,\n",
       " 'recepies_list_path': 'data/models_recepies.json',\n",
       " 'cuda': True,\n",
       " 'batch_size': 20,\n",
       " 'model': 'CustomRNN',\n",
       " 'emsize': 400,\n",
       " 'nhid': 600,\n",
       " 'nlayers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'dropouth': 0.25,\n",
       " 'dropouti': 0.4,\n",
       " 'dropoute': 0.0,\n",
       " 'wdrop': 0.1,\n",
       " 'tied': True,\n",
       " 'bptt': 70,\n",
       " 'lr': 0.001,\n",
       " 'wdecay': 1.2e-06,\n",
       " 'epochs': 50,\n",
       " 'alpha': 2,\n",
       " 'beta': 1,\n",
       " 'log_interval': 200,\n",
       " 'clip': 0.25,\n",
       " 'eval_batch_size': 50,\n",
       " 'recepie': '{\"i\": {\"op\": \"linear\", \"input\": [\"x\", \"h_prev_0\"]}, \"i_act\": {\"op\": \"activation_tanh\", \"input\": [\"i\"]}, \"j\": {\"op\": \"linear\", \"input\": [\"x\", \"h_prev_0\"]}, \"j_act\": {\"op\": \"activation_sigm\", \"input\": [\"j\"]}, \"f\": {\"op\": \"linear\", \"input\": [\"x\", \"h_prev_0\"]}, \"f_act\": {\"op\": \"activation_sigm\", \"input\": [\"f\"]}, \"o\": {\"op\": \"linear\", \"input\": [\"x\", \"h_prev_0\"]}, \"o_act\": {\"op\": \"activation_tanh\", \"input\": [\"o\"]}, \"h_new_1_part1\": {\"op\": \"elementwise_prod\", \"input\": [\"f_act\", \"h_prev_1\"]}, \"h_new_1_part2\": {\"op\": \"elementwise_prod\", \"input\": [\"i_act\", \"j_act\"]}, \"h_new_1\": {\"op\": \"elementwise_sum\", \"input\": [\"h_new_1_part1\", \"h_new_1_part2\"]}, \"h_new_1_act\": {\"op\": \"activation_tanh\", \"input\": [\"h_new_1\"]}, \"h_new_0\": {\"op\": \"elementwise_prod\", \"input\": [\"h_new_1_act\", \"o_act\"]}}',\n",
       " 'experiment_id': 999949254,\n",
       " 'init_time': '2020-04-19_08-20-59',\n",
       " 'num_params': 10896400,\n",
       " 'wall_times': [237.8071584701538,\n",
       "  237.24799275398254,\n",
       "  234.70137882232666,\n",
       "  233.3321213722229,\n",
       "  233.08114004135132,\n",
       "  233.65349817276,\n",
       "  232.94044423103333,\n",
       "  232.984388589859,\n",
       "  232.9627239704132,\n",
       "  230.06033062934875,\n",
       "  231.7639970779419,\n",
       "  233.18582558631897,\n",
       "  233.90077233314514,\n",
       "  233.82443952560425,\n",
       "  233.83285880088806,\n",
       "  233.58099031448364,\n",
       "  233.5606334209442,\n",
       "  233.68089246749878,\n",
       "  233.4503481388092,\n",
       "  233.4342918395996,\n",
       "  233.70389199256897,\n",
       "  233.45750045776367,\n",
       "  233.448317527771,\n",
       "  233.1534206867218,\n",
       "  225.87894439697266,\n",
       "  232.07257175445557,\n",
       "  232.78276252746582,\n",
       "  232.89288759231567,\n",
       "  232.95467233657837,\n",
       "  232.47211909294128,\n",
       "  233.26953768730164,\n",
       "  233.33051896095276,\n",
       "  233.09318017959595,\n",
       "  233.15182948112488,\n",
       "  233.19995641708374,\n",
       "  233.01973581314087,\n",
       "  232.7777237892151,\n",
       "  233.2640027999878,\n",
       "  233.0847852230072,\n",
       "  229.680340051651,\n",
       "  232.39011764526367,\n",
       "  233.60439157485962,\n",
       "  233.85507345199585,\n",
       "  233.13133358955383,\n",
       "  233.2684018611908,\n",
       "  232.64543056488037,\n",
       "  232.93256783485413,\n",
       "  232.86862969398499,\n",
       "  233.16914129257202,\n",
       "  233.26800107955933],\n",
       " 'train_losses': [5.530839014845893,\n",
       "  5.0933747344145015,\n",
       "  4.844764015518262,\n",
       "  4.657351843634016,\n",
       "  4.5092664965574745,\n",
       "  4.383823573906729,\n",
       "  4.275650852563068,\n",
       "  4.183458225754397,\n",
       "  4.103120293421548,\n",
       "  4.031729482679792,\n",
       "  3.9717596888279276,\n",
       "  3.9146722540476575,\n",
       "  3.8625506797644023,\n",
       "  3.814081746544027,\n",
       "  3.7728390921682533,\n",
       "  3.7308849880318435,\n",
       "  3.6939038883061697,\n",
       "  3.657634658571352,\n",
       "  3.6230791276693024,\n",
       "  3.5924187106664514,\n",
       "  3.5638737325856598,\n",
       "  3.5329805080415255,\n",
       "  3.50999790725351,\n",
       "  3.4825243061158626,\n",
       "  3.4575458219030715,\n",
       "  3.433883288620838,\n",
       "  3.412125028575655,\n",
       "  3.3908952080307677,\n",
       "  3.3727662662713143,\n",
       "  3.351195008673552,\n",
       "  3.3304118928513797,\n",
       "  3.3117066053466733,\n",
       "  3.2960288664541983,\n",
       "  3.27765997323974,\n",
       "  3.262859464929267,\n",
       "  3.244459474678608,\n",
       "  3.227208814076704,\n",
       "  3.212789454574794,\n",
       "  3.201826698738637,\n",
       "  3.1847502235624763,\n",
       "  3.1718096542009575,\n",
       "  3.155769256629552,\n",
       "  3.146233602616858,\n",
       "  3.1337386874025066,\n",
       "  3.1224143234629658,\n",
       "  3.1073244866467644,\n",
       "  3.0997168488784896,\n",
       "  3.0861507668361035,\n",
       "  3.0736272759668655,\n",
       "  3.0601817999973107],\n",
       " 'val_losses': [5.564831170550847,\n",
       "  5.190218154131356,\n",
       "  4.9954237288135594,\n",
       "  4.86140029131356,\n",
       "  4.76994968220339,\n",
       "  4.696279131355932,\n",
       "  4.641849841101695,\n",
       "  4.59412705243644,\n",
       "  4.56436076536017,\n",
       "  4.5404197563559325,\n",
       "  4.5204806673728815,\n",
       "  4.5037566207627115,\n",
       "  4.489889102224576,\n",
       "  4.47759765625,\n",
       "  4.4683401747881355,\n",
       "  4.4597646318855935,\n",
       "  4.451193723516949,\n",
       "  4.451774695444915,\n",
       "  4.445090042372882,\n",
       "  4.444392876059322,\n",
       "  4.436959083686441,\n",
       "  4.43800251588983,\n",
       "  4.430377052436441,\n",
       "  4.427646318855932,\n",
       "  4.429302833686441,\n",
       "  4.4255872616525425,\n",
       "  4.426403601694915,\n",
       "  4.421150026483051,\n",
       "  4.419242584745763,\n",
       "  4.415819981461865,\n",
       "  4.421621424788135,\n",
       "  4.419203853283898,\n",
       "  4.419066472457627,\n",
       "  4.415204250529661,\n",
       "  4.4157259666313555,\n",
       "  4.412297735699153,\n",
       "  4.418014433262712,\n",
       "  4.417901549258475,\n",
       "  4.41514797404661,\n",
       "  4.408801641949153,\n",
       "  4.411461533368644,\n",
       "  4.415899099576271,\n",
       "  4.408835076800847,\n",
       "  4.411238413665254,\n",
       "  4.41195014565678,\n",
       "  4.417937301377119,\n",
       "  4.413710606461865,\n",
       "  4.415193657309322,\n",
       "  4.413221332097458,\n",
       "  4.41530952065678],\n",
       " 'test_losses': [5.528548453618022,\n",
       "  5.150762169106493,\n",
       "  4.961021941842385,\n",
       "  4.829225114248331,\n",
       "  4.734429220551426,\n",
       "  4.662982533278974,\n",
       "  4.607904230506675,\n",
       "  4.564316240329187,\n",
       "  4.532639290522603,\n",
       "  4.507565100216171,\n",
       "  4.48360850510088,\n",
       "  4.465394547842081,\n",
       "  4.452875229918841,\n",
       "  4.440619607573574,\n",
       "  4.431249525940534,\n",
       "  4.420638593655188,\n",
       "  4.412994681052791,\n",
       "  4.407892616049757,\n",
       "  4.402103461108162,\n",
       "  4.400656690875303,\n",
       "  4.39306403595267,\n",
       "  4.394185186589806,\n",
       "  4.384678220285953,\n",
       "  4.38266820815003,\n",
       "  4.385711077347542,\n",
       "  4.379345940154733,\n",
       "  4.379940292210255,\n",
       "  4.37678987077139,\n",
       "  4.374247134310528,\n",
       "  4.370828276699029,\n",
       "  4.374431128640777,\n",
       "  4.371352408696147,\n",
       "  4.372530150182039,\n",
       "  4.372374895706917,\n",
       "  4.372517706121056,\n",
       "  4.369250843825849,\n",
       "  4.370345921192355,\n",
       "  4.370796277685073,\n",
       "  4.369263287886833,\n",
       "  4.365811838687045,\n",
       "  4.36680143782236,\n",
       "  4.3696534980848,\n",
       "  4.363588203504247,\n",
       "  4.363264954205856,\n",
       "  4.365649769607099,\n",
       "  4.366507520953428,\n",
       "  4.362638603136378,\n",
       "  4.363554130480128,\n",
       "  4.36434580978838,\n",
       "  4.368057399120145],\n",
       " 'status': 'OK'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stats[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_stats_model_1000001_2020-04-19_08-20-59_999949254.json']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in os.listdir('train_logs_single_run') if x.find('1000001') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '0001_2020-04-19_08-20-59_999949254'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = json.load(open('train_logs_single_run/log_stats_model_100' + suffix + '.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(**log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus(args.data)\n",
    "cuda = 'cuda'\n",
    "\n",
    "train_data = batchify(corpus.train, args.batch_size, args, cuda)\n",
    "train_eval_data = batchify(corpus.train, args.eval_batch_size, args, cuda)\n",
    "val_data = batchify(corpus.valid, args.eval_batch_size, args, cuda)\n",
    "test_data = batchify(corpus.test, args.eval_batch_size, args, cuda)\n",
    "\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = AWDRNNModel(args.model, \n",
    "                               ntokens, \n",
    "                               args.emsize, \n",
    "                               args.nhid, \n",
    "                               args.nlayers, \n",
    "                               args.dropout, \n",
    "                               args.dropouth, \n",
    "                               args.dropouti, \n",
    "                               args.dropoute, \n",
    "                               args.wdrop, \n",
    "                               args.tied,\n",
    "                               args.recepie,\n",
    "                               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.to(cuda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_stats = vars(args)\n",
    "log_stats['experiment_id'] = 'reproduce'\n",
    "log_stats['init_time'] = 'reproduce'\n",
    "log_stats['num_params'] = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] \n",
    "                              for x in custom_model.parameters() if x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SplitCrossEntropyLoss(args.emsize, splits=[], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(custom_model.parameters()) + list(criterion.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.wdecay)\n",
    "\n",
    "lr = args.lr\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "wall_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  663 batches | lr 0.00100 | ms/batch 259.60 | loss  7.09 | ppl  1200.35 | bpc   10.229\n",
      "| epoch   1 |   400/  663 batches | lr 0.00100 | ms/batch 264.24 | loss  6.11 | ppl   448.22 | bpc    8.808\n",
      "| epoch   1 |   600/  663 batches | lr 0.00100 | ms/batch 263.84 | loss  5.79 | ppl   326.57 | bpc    8.351\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 180.45s |\n",
      "| train loss  5.51 | train ppl   248.26 | train bpw    7.956 |\n",
      "| valid loss  5.55 | valid ppl   256.99 | valid bpw    8.006 |\n",
      "| test loss  5.51 | test ppl   247.88 | test bpw    7.953 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  663 batches | lr 0.00100 | ms/batch 267.74 | loss  5.53 | ppl   253.36 | bpc    7.985\n",
      "| epoch   2 |   400/  663 batches | lr 0.00100 | ms/batch 258.11 | loss  5.40 | ppl   220.94 | bpc    7.788\n",
      "| epoch   2 |   600/  663 batches | lr 0.00100 | ms/batch 265.71 | loss  5.28 | ppl   195.86 | bpc    7.614\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 181.93s |\n",
      "| train loss  5.10 | train ppl   163.43 | train bpw    7.352 |\n",
      "| valid loss  5.19 | valid ppl   179.95 | valid bpw    7.491 |\n",
      "| test loss  5.15 | test ppl   172.76 | test bpw    7.433 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  663 batches | lr 0.00100 | ms/batch 260.61 | loss  5.18 | ppl   176.86 | bpc    7.466\n",
      "| epoch   3 |   400/  663 batches | lr 0.00100 | ms/batch 269.25 | loss  5.10 | ppl   164.12 | bpc    7.359\n",
      "| epoch   3 |   600/  663 batches | lr 0.00100 | ms/batch 264.78 | loss  5.02 | ppl   151.72 | bpc    7.245\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 180.53s |\n",
      "| train loss  4.84 | train ppl   126.88 | train bpw    6.987 |\n",
      "| valid loss  4.99 | valid ppl   147.66 | valid bpw    7.206 |\n",
      "| test loss  4.96 | test ppl   142.41 | test bpw    7.154 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  663 batches | lr 0.00100 | ms/batch 267.32 | loss  4.97 | ppl   143.76 | bpc    7.168\n",
      "| epoch   4 |   400/  663 batches | lr 0.00100 | ms/batch 258.63 | loss  4.90 | ppl   134.31 | bpc    7.069\n",
      "| epoch   4 |   600/  663 batches | lr 0.00100 | ms/batch 267.37 | loss  4.84 | ppl   126.15 | bpc    6.979\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 181.86s |\n",
      "| train loss  4.66 | train ppl   105.38 | train bpw    6.719 |\n",
      "| valid loss  4.86 | valid ppl   129.37 | valid bpw    7.015 |\n",
      "| test loss  4.83 | test ppl   125.00 | test bpw    6.966 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  663 batches | lr 0.00100 | ms/batch 263.25 | loss  4.81 | ppl   122.33 | bpc    6.935\n",
      "| epoch   5 |   400/  663 batches | lr 0.00100 | ms/batch 263.05 | loss  4.75 | ppl   115.56 | bpc    6.852\n",
      "| epoch   5 |   600/  663 batches | lr 0.00100 | ms/batch 262.30 | loss  4.70 | ppl   110.06 | bpc    6.782\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 180.70s |\n",
      "| train loss  4.51 | train ppl    90.80 | train bpw    6.505 |\n",
      "| valid loss  4.77 | valid ppl   117.46 | valid bpw    6.876 |\n",
      "| test loss  4.74 | test ppl   113.96 | test bpw    6.832 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  663 batches | lr 0.00100 | ms/batch 268.96 | loss  4.68 | ppl   107.70 | bpc    6.751\n",
      "| epoch   6 |   400/  663 batches | lr 0.00100 | ms/batch 260.64 | loss  4.62 | ppl   101.59 | bpc    6.667\n",
      "| epoch   6 |   600/  663 batches | lr 0.00100 | ms/batch 262.37 | loss  4.59 | ppl    98.51 | bpc    6.622\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 181.84s |\n",
      "| train loss  4.38 | train ppl    79.75 | train bpw    6.317 |\n",
      "| valid loss  4.69 | valid ppl   108.97 | valid bpw    6.768 |\n",
      "| test loss  4.66 | test ppl   105.69 | test bpw    6.724 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  663 batches | lr 0.00100 | ms/batch 259.76 | loss  4.57 | ppl    96.31 | bpc    6.590\n",
      "| epoch   7 |   400/  663 batches | lr 0.00100 | ms/batch 260.93 | loss  4.51 | ppl    91.38 | bpc    6.514\n",
      "| epoch   7 |   600/  663 batches | lr 0.00100 | ms/batch 268.55 | loss  4.50 | ppl    89.94 | bpc    6.491\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 180.57s |\n",
      "| train loss  4.27 | train ppl    71.59 | train bpw    6.162 |\n",
      "| valid loss  4.64 | valid ppl   103.09 | valid bpw    6.688 |\n",
      "| test loss  4.60 | test ppl    99.93 | test bpw    6.643 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  663 batches | lr 0.00100 | ms/batch 264.49 | loss  4.48 | ppl    88.47 | bpc    6.467\n",
      "| epoch   8 |   400/  663 batches | lr 0.00100 | ms/batch 260.64 | loss  4.44 | ppl    84.37 | bpc    6.399\n",
      "| epoch   8 |   600/  663 batches | lr 0.00100 | ms/batch 268.41 | loss  4.42 | ppl    83.14 | bpc    6.377\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 182.41s |\n",
      "| train loss  4.18 | train ppl    65.32 | train bpw    6.030 |\n",
      "| valid loss  4.59 | valid ppl    98.94 | valid bpw    6.628 |\n",
      "| test loss  4.56 | test ppl    95.82 | test bpw    6.582 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  663 batches | lr 0.00100 | ms/batch 261.00 | loss  4.41 | ppl    82.08 | bpc    6.359\n",
      "| epoch   9 |   400/  663 batches | lr 0.00100 | ms/batch 272.68 | loss  4.36 | ppl    78.19 | bpc    6.289\n",
      "| epoch   9 |   600/  663 batches | lr 0.00100 | ms/batch 260.09 | loss  4.36 | ppl    78.08 | bpc    6.287\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 180.41s |\n",
      "| train loss  4.10 | train ppl    60.22 | train bpw    5.912 |\n",
      "| valid loss  4.56 | valid ppl    95.38 | valid bpw    6.576 |\n",
      "| test loss  4.53 | test ppl    92.56 | test bpw    6.532 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  663 batches | lr 0.00100 | ms/batch 269.17 | loss  4.35 | ppl    77.17 | bpc    6.270\n",
      "| epoch  10 |   400/  663 batches | lr 0.00100 | ms/batch 260.93 | loss  4.30 | ppl    73.64 | bpc    6.202\n",
      "| epoch  10 |   600/  663 batches | lr 0.00100 | ms/batch 267.47 | loss  4.30 | ppl    73.49 | bpc    6.199\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 181.92s |\n",
      "| train loss  4.03 | train ppl    56.08 | train bpw    5.809 |\n",
      "| valid loss  4.53 | valid ppl    93.02 | valid bpw    6.539 |\n",
      "| test loss  4.50 | test ppl    89.99 | test bpw    6.492 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  663 batches | lr 0.00100 | ms/batch 260.21 | loss  4.29 | ppl    72.80 | bpc    6.186\n",
      "| epoch  11 |   400/  663 batches | lr 0.00100 | ms/batch 267.02 | loss  4.24 | ppl    69.70 | bpc    6.123\n",
      "| epoch  11 |   600/  663 batches | lr 0.00100 | ms/batch 256.54 | loss  4.25 | ppl    69.82 | bpc    6.126\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 180.71s |\n",
      "| train loss  3.97 | train ppl    52.82 | train bpw    5.723 |\n",
      "| valid loss  4.51 | valid ppl    91.26 | valid bpw    6.512 |\n",
      "| test loss  4.48 | test ppl    88.17 | test bpw    6.462 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  663 batches | lr 0.00100 | ms/batch 266.18 | loss  4.24 | ppl    69.35 | bpc    6.116\n",
      "| epoch  12 |   400/  663 batches | lr 0.00100 | ms/batch 260.74 | loss  4.20 | ppl    66.68 | bpc    6.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   600/  663 batches | lr 0.00100 | ms/batch 265.59 | loss  4.20 | ppl    66.71 | bpc    6.060\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 181.99s |\n",
      "| train loss  3.91 | train ppl    49.94 | train bpw    5.642 |\n",
      "| valid loss  4.50 | valid ppl    89.99 | valid bpw    6.492 |\n",
      "| test loss  4.46 | test ppl    86.78 | test bpw    6.439 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  663 batches | lr 0.00100 | ms/batch 263.94 | loss  4.20 | ppl    66.36 | bpc    6.052\n",
      "| epoch  13 |   400/  663 batches | lr 0.00100 | ms/batch 266.18 | loss  4.15 | ppl    63.69 | bpc    5.993\n",
      "| epoch  13 |   600/  663 batches | lr 0.00100 | ms/batch 258.34 | loss  4.16 | ppl    64.15 | bpc    6.003\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 180.59s |\n",
      "| train loss  3.86 | train ppl    47.36 | train bpw    5.566 |\n",
      "| valid loss  4.49 | valid ppl    88.89 | valid bpw    6.474 |\n",
      "| test loss  4.45 | test ppl    85.72 | test bpw    6.422 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  663 batches | lr 0.00100 | ms/batch 264.72 | loss  4.16 | ppl    63.87 | bpc    5.997\n",
      "| epoch  14 |   400/  663 batches | lr 0.00100 | ms/batch 260.87 | loss  4.12 | ppl    61.71 | bpc    5.947\n",
      "| epoch  14 |   600/  663 batches | lr 0.00100 | ms/batch 269.87 | loss  4.12 | ppl    61.37 | bpc    5.939\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 182.12s |\n",
      "| train loss  3.81 | train ppl    45.18 | train bpw    5.497 |\n",
      "| valid loss  4.48 | valid ppl    87.81 | valid bpw    6.456 |\n",
      "| test loss  4.44 | test ppl    84.69 | test bpw    6.404 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  663 batches | lr 0.00100 | ms/batch 261.87 | loss  4.12 | ppl    61.48 | bpc    5.942\n",
      "| epoch  15 |   400/  663 batches | lr 0.00100 | ms/batch 259.88 | loss  4.08 | ppl    59.36 | bpc    5.891\n",
      "| epoch  15 |   600/  663 batches | lr 0.00100 | ms/batch 266.83 | loss  4.09 | ppl    59.66 | bpc    5.899\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 180.58s |\n",
      "| train loss  3.77 | train ppl    43.24 | train bpw    5.434 |\n",
      "| valid loss  4.47 | valid ppl    87.15 | valid bpw    6.446 |\n",
      "| test loss  4.43 | test ppl    83.78 | test bpw    6.388 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  663 batches | lr 0.00100 | ms/batch 265.10 | loss  4.09 | ppl    59.53 | bpc    5.896\n",
      "| epoch  16 |   400/  663 batches | lr 0.00100 | ms/batch 260.92 | loss  4.05 | ppl    57.53 | bpc    5.846\n",
      "| epoch  16 |   600/  663 batches | lr 0.00100 | ms/batch 262.93 | loss  4.06 | ppl    57.96 | bpc    5.857\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 180.68s |\n",
      "| train loss  3.73 | train ppl    41.57 | train bpw    5.377 |\n",
      "| valid loss  4.46 | valid ppl    86.17 | valid bpw    6.429 |\n",
      "| test loss  4.42 | test ppl    83.07 | test bpw    6.376 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  663 batches | lr 0.00100 | ms/batch 257.49 | loss  4.05 | ppl    57.65 | bpc    5.849\n",
      "| epoch  17 |   400/  663 batches | lr 0.00100 | ms/batch 267.90 | loss  4.02 | ppl    55.80 | bpc    5.802\n",
      "| epoch  17 |   600/  663 batches | lr 0.00100 | ms/batch 260.90 | loss  4.03 | ppl    56.19 | bpc    5.812\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 180.56s |\n",
      "| train loss  3.69 | train ppl    39.98 | train bpw    5.321 |\n",
      "| valid loss  4.45 | valid ppl    85.83 | valid bpw    6.423 |\n",
      "| test loss  4.41 | test ppl    82.51 | test bpw    6.367 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  663 batches | lr 0.00100 | ms/batch 271.39 | loss  4.03 | ppl    56.32 | bpc    5.815\n",
      "| epoch  18 |   400/  663 batches | lr 0.00100 | ms/batch 251.96 | loss  3.99 | ppl    54.14 | bpc    5.759\n",
      "| epoch  18 |   600/  663 batches | lr 0.00100 | ms/batch 274.98 | loss  4.00 | ppl    54.78 | bpc    5.776\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 180.81s |\n",
      "| train loss  3.66 | train ppl    38.69 | train bpw    5.274 |\n",
      "| valid loss  4.45 | valid ppl    85.37 | valid bpw    6.416 |\n",
      "| test loss  4.41 | test ppl    82.14 | test bpw    6.360 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  663 batches | lr 0.00100 | ms/batch 266.21 | loss  4.01 | ppl    55.02 | bpc    5.782\n",
      "| epoch  19 |   400/  663 batches | lr 0.00100 | ms/batch 267.35 | loss  3.96 | ppl    52.32 | bpc    5.709\n",
      "| epoch  19 |   600/  663 batches | lr 0.00100 | ms/batch 259.80 | loss  3.98 | ppl    53.64 | bpc    5.745\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 180.56s |\n",
      "| train loss  3.62 | train ppl    37.32 | train bpw    5.222 |\n",
      "| valid loss  4.44 | valid ppl    84.73 | valid bpw    6.405 |\n",
      "| test loss  4.40 | test ppl    81.58 | test bpw    6.350 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  663 batches | lr 0.00100 | ms/batch 271.67 | loss  3.97 | ppl    53.22 | bpc    5.734\n",
      "| epoch  20 |   400/  663 batches | lr 0.00100 | ms/batch 259.58 | loss  3.94 | ppl    51.64 | bpc    5.690\n",
      "| epoch  20 |   600/  663 batches | lr 0.00100 | ms/batch 269.98 | loss  3.95 | ppl    52.03 | bpc    5.701\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 181.91s |\n",
      "| train loss  3.59 | train ppl    36.15 | train bpw    5.176 |\n",
      "| valid loss  4.43 | valid ppl    84.21 | valid bpw    6.396 |\n",
      "| test loss  4.39 | test ppl    80.95 | test bpw    6.339 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  663 batches | lr 0.00100 | ms/batch 261.19 | loss  3.96 | ppl    52.37 | bpc    5.711\n",
      "| epoch  21 |   400/  663 batches | lr 0.00100 | ms/batch 264.36 | loss  3.92 | ppl    50.54 | bpc    5.659\n",
      "| epoch  21 |   600/  663 batches | lr 0.00100 | ms/batch 262.52 | loss  3.93 | ppl    50.94 | bpc    5.671\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 180.73s |\n",
      "| train loss  3.56 | train ppl    35.11 | train bpw    5.134 |\n",
      "| valid loss  4.43 | valid ppl    83.69 | valid bpw    6.387 |\n",
      "| test loss  4.39 | test ppl    80.80 | test bpw    6.336 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  663 batches | lr 0.00100 | ms/batch 269.30 | loss  3.93 | ppl    51.15 | bpc    5.677\n",
      "| epoch  22 |   400/  663 batches | lr 0.00100 | ms/batch 262.76 | loss  3.90 | ppl    49.43 | bpc    5.627\n",
      "| epoch  22 |   600/  663 batches | lr 0.00100 | ms/batch 264.86 | loss  3.91 | ppl    49.93 | bpc    5.642\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 180.53s |\n",
      "| train loss  3.53 | train ppl    34.07 | train bpw    5.090 |\n",
      "| valid loss  4.42 | valid ppl    83.30 | valid bpw    6.380 |\n",
      "| test loss  4.39 | test ppl    80.49 | test bpw    6.331 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  663 batches | lr 0.00100 | ms/batch 263.61 | loss  3.91 | ppl    50.15 | bpc    5.648\n",
      "| epoch  23 |   400/  663 batches | lr 0.00100 | ms/batch 268.24 | loss  3.88 | ppl    48.50 | bpc    5.600\n",
      "| epoch  23 |   600/  663 batches | lr 0.00100 | ms/batch 261.49 | loss  3.89 | ppl    48.95 | bpc    5.613\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 181.90s |\n",
      "| train loss  3.50 | train ppl    33.13 | train bpw    5.050 |\n",
      "| valid loss  4.42 | valid ppl    83.47 | valid bpw    6.383 |\n",
      "| test loss  4.39 | test ppl    80.32 | test bpw    6.328 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  24 |   200/  663 batches | lr 0.00100 | ms/batch 272.34 | loss  3.89 | ppl    48.99 | bpc    5.614\n",
      "| epoch  24 |   400/  663 batches | lr 0.00100 | ms/batch 261.05 | loss  3.85 | ppl    47.17 | bpc    5.560\n",
      "| epoch  24 |   600/  663 batches | lr 0.00100 | ms/batch 249.50 | loss  3.87 | ppl    47.92 | bpc    5.583\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 175.65s |\n",
      "| train loss  3.47 | train ppl    32.23 | train bpw    5.010 |\n",
      "| valid loss  4.42 | valid ppl    82.91 | valid bpw    6.373 |\n",
      "| test loss  4.38 | test ppl    80.06 | test bpw    6.323 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  663 batches | lr 0.00100 | ms/batch 244.08 | loss  3.87 | ppl    47.87 | bpc    5.581\n",
      "| epoch  25 |   400/  663 batches | lr 0.00100 | ms/batch 238.94 | loss  3.84 | ppl    46.63 | bpc    5.543\n",
      "| epoch  25 |   600/  663 batches | lr 0.00100 | ms/batch 245.97 | loss  3.85 | ppl    47.03 | bpc    5.555\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 165.69s |\n",
      "| train loss  3.45 | train ppl    31.61 | train bpw    4.982 |\n",
      "| valid loss  4.42 | valid ppl    82.71 | valid bpw    6.370 |\n",
      "| test loss  4.38 | test ppl    79.72 | test bpw    6.317 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/  663 batches | lr 0.00100 | ms/batch 250.60 | loss  3.86 | ppl    47.39 | bpc    5.567\n",
      "| epoch  26 |   400/  663 batches | lr 0.00100 | ms/batch 258.56 | loss  3.82 | ppl    45.69 | bpc    5.514\n",
      "| epoch  26 |   600/  663 batches | lr 0.00100 | ms/batch 267.71 | loss  3.83 | ppl    46.20 | bpc    5.530\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 178.32s |\n",
      "| train loss  3.43 | train ppl    30.82 | train bpw    4.946 |\n",
      "| valid loss  4.41 | valid ppl    82.34 | valid bpw    6.363 |\n",
      "| test loss  4.38 | test ppl    79.51 | test bpw    6.313 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  663 batches | lr 0.00100 | ms/batch 261.27 | loss  3.84 | ppl    46.44 | bpc    5.537\n",
      "| epoch  27 |   400/  663 batches | lr 0.00100 | ms/batch 264.05 | loss  3.81 | ppl    45.08 | bpc    5.494\n",
      "| epoch  27 |   600/  663 batches | lr 0.00100 | ms/batch 259.71 | loss  3.82 | ppl    45.66 | bpc    5.513\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 180.83s |\n",
      "| train loss  3.41 | train ppl    30.27 | train bpw    4.920 |\n",
      "| valid loss  4.41 | valid ppl    82.12 | valid bpw    6.360 |\n",
      "| test loss  4.38 | test ppl    79.45 | test bpw    6.312 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  663 batches | lr 0.00100 | ms/batch 268.85 | loss  3.82 | ppl    45.73 | bpc    5.515\n",
      "| epoch  28 |   400/  663 batches | lr 0.00100 | ms/batch 256.43 | loss  3.79 | ppl    44.33 | bpc    5.470\n",
      "| epoch  28 |   600/  663 batches | lr 0.00100 | ms/batch 263.96 | loss  3.81 | ppl    44.97 | bpc    5.491\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 182.24s |\n",
      "| train loss  3.38 | train ppl    29.45 | train bpw    4.880 |\n",
      "| valid loss  4.41 | valid ppl    82.15 | valid bpw    6.360 |\n",
      "| test loss  4.37 | test ppl    79.36 | test bpw    6.310 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  663 batches | lr 0.00100 | ms/batch 259.79 | loss  3.81 | ppl    45.05 | bpc    5.494\n",
      "| epoch  29 |   400/  663 batches | lr 0.00100 | ms/batch 265.43 | loss  3.78 | ppl    43.86 | bpc    5.455\n",
      "| epoch  29 |   600/  663 batches | lr 0.00100 | ms/batch 263.12 | loss  3.80 | ppl    44.52 | bpc    5.476\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 180.71s |\n",
      "| train loss  3.36 | train ppl    28.92 | train bpw    4.854 |\n",
      "| valid loss  4.40 | valid ppl    81.78 | valid bpw    6.354 |\n",
      "| test loss  4.37 | test ppl    79.05 | test bpw    6.305 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  663 batches | lr 0.00100 | ms/batch 267.75 | loss  3.79 | ppl    44.46 | bpc    5.474\n",
      "| epoch  30 |   400/  663 batches | lr 0.00100 | ms/batch 259.06 | loss  3.76 | ppl    43.02 | bpc    5.427\n",
      "| epoch  30 |   600/  663 batches | lr 0.00100 | ms/batch 269.54 | loss  3.77 | ppl    43.58 | bpc    5.446\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 181.98s |\n",
      "| train loss  3.34 | train ppl    28.26 | train bpw    4.821 |\n",
      "| valid loss  4.41 | valid ppl    82.04 | valid bpw    6.358 |\n",
      "| test loss  4.37 | test ppl    79.18 | test bpw    6.307 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/  663 batches | lr 0.00100 | ms/batch 264.03 | loss  3.78 | ppl    43.98 | bpc    5.459\n",
      "| epoch  31 |   400/  663 batches | lr 0.00100 | ms/batch 265.96 | loss  3.75 | ppl    42.39 | bpc    5.406\n",
      "| epoch  31 |   600/  663 batches | lr 0.00100 | ms/batch 254.54 | loss  3.76 | ppl    43.11 | bpc    5.430\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 180.72s |\n",
      "| train loss  3.32 | train ppl    27.77 | train bpw    4.796 |\n",
      "| valid loss  4.40 | valid ppl    81.51 | valid bpw    6.349 |\n",
      "| test loss  4.37 | test ppl    78.81 | test bpw    6.300 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/  663 batches | lr 0.00100 | ms/batch 269.85 | loss  3.76 | ppl    43.11 | bpc    5.430\n",
      "| epoch  32 |   400/  663 batches | lr 0.00100 | ms/batch 261.38 | loss  3.74 | ppl    42.05 | bpc    5.394\n",
      "| epoch  32 |   600/  663 batches | lr 0.00100 | ms/batch 268.79 | loss  3.75 | ppl    42.71 | bpc    5.416\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 181.98s |\n",
      "| train loss  3.30 | train ppl    27.24 | train bpw    4.768 |\n",
      "| valid loss  4.40 | valid ppl    81.23 | valid bpw    6.344 |\n",
      "| test loss  4.37 | test ppl    78.78 | test bpw    6.300 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/  663 batches | lr 0.00100 | ms/batch 262.48 | loss  3.75 | ppl    42.45 | bpc    5.408\n",
      "| epoch  33 |   400/  663 batches | lr 0.00100 | ms/batch 264.66 | loss  3.73 | ppl    41.55 | bpc    5.377\n",
      "| epoch  33 |   600/  663 batches | lr 0.00100 | ms/batch 262.21 | loss  3.74 | ppl    41.91 | bpc    5.389\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 180.78s |\n",
      "| train loss  3.29 | train ppl    26.71 | train bpw    4.740 |\n",
      "| valid loss  4.40 | valid ppl    81.31 | valid bpw    6.345 |\n",
      "| test loss  4.37 | test ppl    78.70 | test bpw    6.298 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/  663 batches | lr 0.00100 | ms/batch 267.95 | loss  3.74 | ppl    41.97 | bpc    5.391\n",
      "| epoch  34 |   400/  663 batches | lr 0.00100 | ms/batch 258.54 | loss  3.71 | ppl    40.86 | bpc    5.353\n",
      "| epoch  34 |   600/  663 batches | lr 0.00100 | ms/batch 267.38 | loss  3.73 | ppl    41.64 | bpc    5.380\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 182.03s |\n",
      "| train loss  3.27 | train ppl    26.27 | train bpw    4.716 |\n",
      "| valid loss  4.40 | valid ppl    81.66 | valid bpw    6.352 |\n",
      "| test loss  4.37 | test ppl    78.92 | test bpw    6.302 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/  663 batches | lr 0.00100 | ms/batch 260.07 | loss  3.73 | ppl    41.47 | bpc    5.374\n",
      "| epoch  35 |   400/  663 batches | lr 0.00100 | ms/batch 267.57 | loss  3.70 | ppl    40.37 | bpc    5.335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  35 |   600/  663 batches | lr 0.00100 | ms/batch 261.21 | loss  3.71 | ppl    41.01 | bpc    5.358\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 180.64s |\n",
      "| train loss  3.25 | train ppl    25.86 | train bpw    4.692 |\n",
      "| valid loss  4.40 | valid ppl    81.33 | valid bpw    6.346 |\n",
      "| test loss  4.36 | test ppl    78.64 | test bpw    6.297 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/  663 batches | lr 0.00100 | ms/batch 267.73 | loss  3.71 | ppl    40.98 | bpc    5.357\n",
      "| epoch  36 |   400/  663 batches | lr 0.00100 | ms/batch 259.39 | loss  3.69 | ppl    40.08 | bpc    5.325\n",
      "| epoch  36 |   600/  663 batches | lr 0.00100 | ms/batch 264.86 | loss  3.70 | ppl    40.48 | bpc    5.339\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 180.59s |\n",
      "| train loss  3.24 | train ppl    25.46 | train bpw    4.670 |\n",
      "| valid loss  4.40 | valid ppl    81.63 | valid bpw    6.351 |\n",
      "| test loss  4.37 | test ppl    78.83 | test bpw    6.301 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/  663 batches | lr 0.00100 | ms/batch 256.85 | loss  3.70 | ppl    40.63 | bpc    5.345\n",
      "| epoch  37 |   400/  663 batches | lr 0.00100 | ms/batch 264.69 | loss  3.67 | ppl    39.44 | bpc    5.302\n",
      "| epoch  37 |   600/  663 batches | lr 0.00100 | ms/batch 260.05 | loss  3.69 | ppl    39.99 | bpc    5.322\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 180.81s |\n",
      "| train loss  3.22 | train ppl    24.95 | train bpw    4.641 |\n",
      "| valid loss  4.40 | valid ppl    81.34 | valid bpw    6.346 |\n",
      "| test loss  4.36 | test ppl    78.38 | test bpw    6.292 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/  663 batches | lr 0.00100 | ms/batch 268.57 | loss  3.69 | ppl    40.15 | bpc    5.327\n",
      "| epoch  38 |   400/  663 batches | lr 0.00100 | ms/batch 260.51 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
      "| epoch  38 |   600/  663 batches | lr 0.00100 | ms/batch 269.48 | loss  3.68 | ppl    39.65 | bpc    5.309\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 179.49s |\n",
      "| train loss  3.21 | train ppl    24.66 | train bpw    4.624 |\n",
      "| valid loss  4.40 | valid ppl    81.12 | valid bpw    6.342 |\n",
      "| test loss  4.36 | test ppl    78.34 | test bpw    6.292 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/  663 batches | lr 0.00100 | ms/batch 266.97 | loss  3.69 | ppl    39.97 | bpc    5.321\n",
      "| epoch  39 |   400/  663 batches | lr 0.00100 | ms/batch 265.70 | loss  3.65 | ppl    38.60 | bpc    5.270\n",
      "| epoch  39 |   600/  663 batches | lr 0.00100 | ms/batch 258.10 | loss  3.67 | ppl    39.38 | bpc    5.299\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 180.51s |\n",
      "| train loss  3.19 | train ppl    24.22 | train bpw    4.598 |\n",
      "| valid loss  4.40 | valid ppl    81.29 | valid bpw    6.345 |\n",
      "| test loss  4.36 | test ppl    78.63 | test bpw    6.297 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/  663 batches | lr 0.00100 | ms/batch 263.95 | loss  3.67 | ppl    39.40 | bpc    5.300\n",
      "| epoch  40 |   400/  663 batches | lr 0.00100 | ms/batch 266.82 | loss  3.65 | ppl    38.32 | bpc    5.260\n",
      "| epoch  40 |   600/  663 batches | lr 0.00100 | ms/batch 264.17 | loss  3.66 | ppl    38.92 | bpc    5.282\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 180.38s |\n",
      "| train loss  3.17 | train ppl    23.87 | train bpw    4.577 |\n",
      "| valid loss  4.40 | valid ppl    81.23 | valid bpw    6.344 |\n",
      "| test loss  4.36 | test ppl    78.49 | test bpw    6.294 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   200/  663 batches | lr 0.00100 | ms/batch 265.85 | loss  3.66 | ppl    39.04 | bpc    5.287\n",
      "| epoch  41 |   400/  663 batches | lr 0.00100 | ms/batch 255.60 | loss  3.63 | ppl    37.72 | bpc    5.237\n",
      "| epoch  41 |   600/  663 batches | lr 0.00100 | ms/batch 265.92 | loss  3.65 | ppl    38.37 | bpc    5.262\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 180.41s |\n",
      "| train loss  3.16 | train ppl    23.50 | train bpw    4.554 |\n",
      "| valid loss  4.39 | valid ppl    81.02 | valid bpw    6.340 |\n",
      "| test loss  4.36 | test ppl    78.34 | test bpw    6.292 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |   200/  663 batches | lr 0.00100 | ms/batch 255.87 | loss  3.65 | ppl    38.63 | bpc    5.272\n",
      "| epoch  42 |   400/  663 batches | lr 0.00100 | ms/batch 265.96 | loss  3.63 | ppl    37.55 | bpc    5.231\n",
      "| epoch  42 |   600/  663 batches | lr 0.00100 | ms/batch 258.08 | loss  3.64 | ppl    38.27 | bpc    5.258\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 177.67s |\n",
      "| train loss  3.14 | train ppl    23.22 | train bpw    4.537 |\n",
      "| valid loss  4.40 | valid ppl    81.09 | valid bpw    6.341 |\n",
      "| test loss  4.36 | test ppl    78.27 | test bpw    6.290 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   200/  663 batches | lr 0.00100 | ms/batch 281.52 | loss  3.65 | ppl    38.31 | bpc    5.260\n",
      "| epoch  43 |   400/  663 batches | lr 0.00100 | ms/batch 256.06 | loss  3.61 | ppl    37.09 | bpc    5.213\n",
      "| epoch  43 |   600/  663 batches | lr 0.00100 | ms/batch 261.29 | loss  3.64 | ppl    37.93 | bpc    5.245\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 181.16s |\n",
      "| train loss  3.13 | train ppl    22.95 | train bpw    4.520 |\n",
      "| valid loss  4.40 | valid ppl    81.16 | valid bpw    6.343 |\n",
      "| test loss  4.36 | test ppl    78.25 | test bpw    6.290 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   200/  663 batches | lr 0.00100 | ms/batch 254.13 | loss  3.63 | ppl    37.87 | bpc    5.243\n",
      "| epoch  44 |   400/  663 batches | lr 0.00100 | ms/batch 264.33 | loss  3.60 | ppl    36.77 | bpc    5.200\n",
      "| epoch  44 |   600/  663 batches | lr 0.00100 | ms/batch 259.66 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 178.04s |\n",
      "| train loss  3.12 | train ppl    22.59 | train bpw    4.498 |\n",
      "| valid loss  4.40 | valid ppl    81.31 | valid bpw    6.345 |\n",
      "| test loss  4.36 | test ppl    78.45 | test bpw    6.294 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |   200/  663 batches | lr 0.00100 | ms/batch 257.61 | loss  3.63 | ppl    37.69 | bpc    5.236\n",
      "| epoch  45 |   400/  663 batches | lr 0.00100 | ms/batch 258.27 | loss  3.60 | ppl    36.66 | bpc    5.196\n",
      "| epoch  45 |   600/  663 batches | lr 0.00100 | ms/batch 260.23 | loss  3.61 | ppl    37.10 | bpc    5.213\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 177.51s |\n",
      "| train loss  3.11 | train ppl    22.31 | train bpw    4.480 |\n",
      "| valid loss  4.40 | valid ppl    81.30 | valid bpw    6.345 |\n",
      "| test loss  4.36 | test ppl    78.50 | test bpw    6.295 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   200/  663 batches | lr 0.00100 | ms/batch 264.07 | loss  3.62 | ppl    37.30 | bpc    5.221\n",
      "| epoch  46 |   400/  663 batches | lr 0.00100 | ms/batch 255.70 | loss  3.59 | ppl    36.24 | bpc    5.180\n",
      "| epoch  46 |   600/  663 batches | lr 0.00100 | ms/batch 263.52 | loss  3.61 | ppl    36.96 | bpc    5.208\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 178.83s |\n",
      "| train loss  3.10 | train ppl    22.12 | train bpw    4.467 |\n",
      "| valid loss  4.40 | valid ppl    81.09 | valid bpw    6.341 |\n",
      "| test loss  4.36 | test ppl    78.44 | test bpw    6.294 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  47 |   200/  663 batches | lr 0.00100 | ms/batch 257.33 | loss  3.61 | ppl    37.00 | bpc    5.210\n",
      "| epoch  47 |   400/  663 batches | lr 0.00100 | ms/batch 253.04 | loss  3.58 | ppl    35.84 | bpc    5.164\n",
      "| epoch  47 |   600/  663 batches | lr 0.00100 | ms/batch 263.67 | loss  3.60 | ppl    36.42 | bpc    5.187\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 177.35s |\n",
      "| train loss  3.08 | train ppl    21.82 | train bpw    4.447 |\n",
      "| valid loss  4.39 | valid ppl    80.87 | valid bpw    6.337 |\n",
      "| test loss  4.36 | test ppl    78.35 | test bpw    6.292 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |   200/  663 batches | lr 0.00100 | ms/batch 255.21 | loss  3.60 | ppl    36.66 | bpc    5.196\n",
      "| epoch  48 |   400/  663 batches | lr 0.00100 | ms/batch 259.28 | loss  3.57 | ppl    35.63 | bpc    5.155\n",
      "| epoch  48 |   600/  663 batches | lr 0.00100 | ms/batch 261.29 | loss  3.59 | ppl    36.18 | bpc    5.177\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 177.65s |\n",
      "| train loss  3.07 | train ppl    21.55 | train bpw    4.430 |\n",
      "| valid loss  4.40 | valid ppl    81.13 | valid bpw    6.342 |\n",
      "| test loss  4.36 | test ppl    78.49 | test bpw    6.294 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |   200/  663 batches | lr 0.00100 | ms/batch 265.12 | loss  3.60 | ppl    36.46 | bpc    5.188\n",
      "| epoch  49 |   400/  663 batches | lr 0.00100 | ms/batch 255.33 | loss  3.56 | ppl    35.26 | bpc    5.140\n",
      "| epoch  49 |   600/  663 batches | lr 0.00100 | ms/batch 263.66 | loss  3.59 | ppl    36.17 | bpc    5.177\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 178.68s |\n",
      "| train loss  3.06 | train ppl    21.32 | train bpw    4.414 |\n",
      "| valid loss  4.40 | valid ppl    81.35 | valid bpw    6.346 |\n",
      "| test loss  4.37 | test ppl    78.68 | test bpw    6.298 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |   200/  663 batches | lr 0.00100 | ms/batch 254.64 | loss  3.59 | ppl    36.13 | bpc    5.175\n",
      "| epoch  50 |   400/  663 batches | lr 0.00100 | ms/batch 262.15 | loss  3.56 | ppl    35.15 | bpc    5.135\n",
      "| epoch  50 |   600/  663 batches | lr 0.00100 | ms/batch 257.28 | loss  3.57 | ppl    35.66 | bpc    5.156\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 177.33s |\n",
      "| train loss  3.05 | train ppl    21.15 | train bpw    4.403 |\n",
      "| valid loss  4.39 | valid ppl    80.87 | valid bpw    6.338 |\n",
      "| test loss  4.36 | test ppl    78.30 | test bpw    6.291 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(custom_model, optimizer, params, criterion, train_data, args, epoch)\n",
    "    epoch_end_time = time.time()\n",
    "    train_loss = evaluate(custom_model, criterion, train_eval_data, args.eval_batch_size, args)\n",
    "    val_loss = evaluate(custom_model, criterion, val_data, args.eval_batch_size, args)\n",
    "    test_loss = evaluate(custom_model, criterion, test_data, args.eval_batch_size, args)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s |\\n| train loss {:5.2f} | '\n",
    "        'train ppl {:8.2f} | train bpw {:8.3f} |\\n| valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f} | valid bpw {:8.3f} |\\n| test loss {:5.2f} | '\n",
    "        'test ppl {:8.2f} | test bpw {:8.3f} |'.format(\n",
    "      epoch, (epoch_end_time - epoch_start_time), \n",
    "            train_loss, math.exp(train_loss), train_loss / math.log(2),\n",
    "            val_loss, math.exp(val_loss), val_loss / math.log(2),\n",
    "        test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    wall_times.append(epoch_end_time - epoch_start_time)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if np.isnan(np.array([train_loss, val_loss, test_loss])).any():\n",
    "        status = 'loss is nan!'\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.load_state_dict(torch.load('models_weights/dump_weights_model_' + suffix + '.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.to(cuda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SplitCrossEntropyLoss(args.emsize, splits=[], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = evaluate(custom_model, criterion, train_eval_data, args.eval_batch_size, args)\n",
    "val_loss = evaluate(custom_model, criterion, val_data, args.eval_batch_size, args)\n",
    "test_loss = evaluate(custom_model, criterion, test_data, args.eval_batch_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 89)\n",
    "print('train loss {:5.4f} | '\n",
    "    'train ppl {:8.2f} | train bpw {:8.3f} |\\n| valid loss {:5.4f} | '\n",
    "    'valid ppl {:8.2f} | valid bpw {:8.3f} |\\n| test loss {:5.4f} | '\n",
    "    'test ppl {:8.2f} | test bpw {:8.3f} |'.format(\n",
    "        train_loss, math.exp(train_loss), train_loss / math.log(2),\n",
    "        val_loss, math.exp(val_loss), val_loss / math.log(2),\n",
    "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
    "print('-' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logged train loss', log['train_losses'][-1])\n",
    "print('logged valid loss', log['val_losses'][-1])\n",
    "print('logged test loss', log['test_losses'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
